{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837edc26-6f2f-4c82-943d-3545ecaeb52d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1 PDF转化\n",
    "由于提取代码使用PDF提取效果较好，使用topdf.py将ppt等文件转化为PDF即可，使用过程中仅需输入仅需要输入文件夹路径即可，代码如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd4145-1f5b-4541-bfae-8b3c996aa2a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 使用前pip install os comtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e8ef91b-3ec6-4265-b4e2-a0715aa5082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from comtypes.client import CreateObject\n",
    "\n",
    "def convert_to_pdf(input_path, output_path, file_type):\n",
    "    try:\n",
    "        if file_type in ['.docx', '.doc']:\n",
    "            word = CreateObject('Word.Application')\n",
    "            doc = word.Documents.Open(input_path)\n",
    "            doc.SaveAs(output_path, FileFormat=17)  # FileFormat=17 corresponds to PDF\n",
    "            doc.Close()\n",
    "            word.Quit()\n",
    "        elif file_type in ['.pptx', '.ppt']:\n",
    "            powerpoint = CreateObject('PowerPoint.Application')\n",
    "            presentation = powerpoint.Presentations.Open(input_path)\n",
    "            presentation.SaveAs(output_path, 32)  # 32 corresponds to PDF\n",
    "            presentation.Close()\n",
    "            powerpoint.Quit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {input_path}: {e}\")\n",
    "\n",
    "def batch_convert(folder_path):\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            if ext in ['.docx', '.doc', '.pptx', '.ppt']:\n",
    "                input_file = os.path.join(root, file)\n",
    "                output_file = os.path.splitext(input_file)[0] + '.pdf'\n",
    "                print(f\"Converting: {input_file} -> {output_file}\")\n",
    "                convert_to_pdf(input_file, output_file, ext)\n",
    "\n",
    "# 替换为你的文件夹路径\n",
    "folder_path = r\"G:\\yuliaoku\\新能源减排服务\"\n",
    "batch_convert(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a689cb-8e8a-442d-a5eb-5447fff85ade",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 PDF读取\n",
    "使用OPNEDATALAB的开源项目MinerU，使用pdf-3.2.py进行PDF提取转化成markdowm格式，使用过程仅需要输入pdf的存放文件夹路径即可，代码如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e866464-7dfc-4d0a-a86f-e725d1b69df0",
   "metadata": {},
   "source": [
    "## 使用前注意事项\n",
    "访问其开源网页https://mineru.readthedocs.io/zh-cn/latest/user_guide/install/install.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d425cf1-6a0c-49b6-9eb9-64acfc997140",
   "metadata": {},
   "source": [
    "### 创建虚拟环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defc133-de93-4a70-9b4a-41875c0aa216",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n MinerU python=3.10\n",
    "conda activate MinerU\n",
    "pip install -U magic-pdf[full] --extra-index-url https://wheels.myhloli.com -i https://mirrors.aliyun.com/pypi/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db626d4f-03dc-48fb-b23b-c6d7934125d7",
   "metadata": {},
   "source": [
    "### 下载权重文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b418f5-6557-4468-a928-86a2463f12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n",
    "wget https://gitee.com/myhloli/MinerU/raw/master/scripts/download_models_hf.py -O download_models_hf.py\n",
    "python download_models_hf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b3ab8-f01f-486d-ac6f-bc1beef593ca",
   "metadata": {},
   "source": [
    "### 使用CUDA加速\n",
    "建议使用cuda加速，显存最低需要8g以上\n",
    "完成下载权重文件步骤后，脚本会自动生成用户目录下的magic-pdf.json文件，并自动配置默认模型路径。您可在【用户目录】下找到magic-pdf.json文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41ad60-a133-47a4-b0d9-a7645194478e",
   "metadata": {},
   "source": [
    "## 问题存在\n",
    "MinerU库中的配置文件不支持多卡去跑，目前只能单卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d40bb-b12e-4acd-80b0-b9d635702d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\n",
    "from magic_pdf.data.dataset import PymuDocDataset\n",
    "from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze\n",
    "from magic_pdf.config.enums import SupportedPdfParseMethod\n",
    "\n",
    "def is_file_processed(pdf_file_name, output_dir):\n",
    "    \"\"\"\n",
    "    检查 PDF 文件是否已经被处理。\n",
    "    Args:\n",
    "        pdf_file_name (str): PDF 文件路径。\n",
    "        output_dir (str): 输出文件夹路径。\n",
    "    Returns:\n",
    "        bool: 如果已处理则返回 True，否则返回 False。\n",
    "    \"\"\"\n",
    "    name_without_suff = os.path.splitext(os.path.basename(pdf_file_name))[0]\n",
    "    md_file_path = os.path.join(output_dir, \"markdown\", f\"{name_without_suff}.md\")\n",
    "    content_list_path = os.path.join(output_dir, \"markdown\", f\"{name_without_suff}_content_list.json\")\n",
    "\n",
    "    return os.path.exists(md_file_path) and os.path.exists(content_list_path)\n",
    "\n",
    "def process_pdf(pdf_file_name, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    处理单个 PDF 文件，支持 OCR 模式和文本模式。\n",
    "    Args:\n",
    "        pdf_file_name (str): 输入的 PDF 文件路径。\n",
    "        output_dir (str): 输出文件夹路径。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 检查文件是否已经被处理\n",
    "        if is_file_processed(pdf_file_name, output_dir):\n",
    "            print(f\"Skipping already processed file: {pdf_file_name}\")\n",
    "            return\n",
    "\n",
    "        # 获取文件名（无后缀）\n",
    "        name_without_suff = os.path.splitext(os.path.basename(pdf_file_name))[0]\n",
    "\n",
    "        # 创建输出目录\n",
    "        local_image_dir = os.path.join(output_dir, \"images\", os.path.splitext(os.path.basename(pdf_file_name))[0])\n",
    "        local_md_dir = os.path.join(output_dir, \"markdown\")\n",
    "        os.makedirs(local_image_dir, exist_ok=True)\n",
    "        os.makedirs(local_md_dir, exist_ok=True)\n",
    "\n",
    "        # 初始化数据写入器\n",
    "        image_writer = FileBasedDataWriter(local_image_dir)\n",
    "        md_writer = FileBasedDataWriter(local_md_dir)\n",
    "\n",
    "        # 读取 PDF 文件的字节流\n",
    "        reader = FileBasedDataReader(\"\")\n",
    "        pdf_bytes = reader.read(pdf_file_name)\n",
    "\n",
    "        # 创建数据集实例\n",
    "        ds = PymuDocDataset(pdf_bytes)\n",
    "\n",
    "        # 推断并分类\n",
    "        if ds.classify() == SupportedPdfParseMethod.OCR:\n",
    "            infer_result = ds.apply(doc_analyze, ocr=True)\n",
    "            pipe_result = infer_result.pipe_ocr_mode(image_writer)\n",
    "        else:\n",
    "            infer_result = ds.apply(doc_analyze, ocr=False)\n",
    "            pipe_result = infer_result.pipe_txt_mode(image_writer)\n",
    "\n",
    "        # 输出结果文件\n",
    "        pipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", os.path.basename(local_image_dir))\n",
    "        pipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", os.path.basename(local_image_dir))\n",
    "\n",
    "        print(f\"Successfully processed: {pdf_file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file_name}: {e}\")\n",
    "\n",
    "def process_folder(input_folder, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    批量处理文件夹中的所有 PDF 文件。\n",
    "    Args:\n",
    "        input_folder (str): 输入文件夹路径。\n",
    "        output_dir (str): 输出文件夹路径。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    skipped_files = []\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_file_path = os.path.join(root, file)\n",
    "                if is_file_processed(pdf_file_path, output_dir):\n",
    "                    skipped_files.append(pdf_file_path)\n",
    "                process_pdf(pdf_file_path, output_dir)\n",
    "    \n",
    "    if skipped_files:\n",
    "        print(\"\\nSkipped files:\")\n",
    "        for skipped_file in skipped_files:\n",
    "            print(skipped_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 输入文件夹和输出文件夹路径\n",
    "    input_folder = \"/home/yancong/xin-llm/第二次拷贝/books\"\n",
    "    output_dir = \"/home/yancong/xin-llm/第二次拷贝/books/output\"\n",
    "\n",
    "    # 批量处理整个文件夹中的 PDF 文件\n",
    "    process_folder(input_folder, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6108a7",
   "metadata": {},
   "source": [
    "### 已修改\n",
    "使用duo.py可以使用多卡去跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def is_file_processed(pdf_file_name, output_dir):\n",
    "    \"\"\"\n",
    "    检查 PDF 文件是否已经被处理。\n",
    "    \"\"\"\n",
    "    name_without_suff = os.path.splitext(os.path.basename(pdf_file_name))[0]\n",
    "    md_file_path = os.path.join(output_dir, \"markdown\", f\"{name_without_suff}.md\")\n",
    "    content_list_path = os.path.join(output_dir, \"markdown\", f\"{name_without_suff}_content_list.json\")\n",
    "    return os.path.exists(md_file_path) and os.path.exists(content_list_path)\n",
    "\n",
    "def process_pdf(pdf_file_name, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    处理单个 PDF 文件，支持 OCR 模式和文本模式。\n",
    "    注意：不要在此函数外部 import torch 或 magic_pdf，而是在此函数内部 import，\n",
    "    这样可以确保子进程已设置好 CUDA_VISIBLE_DEVICES 后再初始化相关库。\n",
    "    \"\"\"\n",
    "    # 只有当我们真正执行到这里时，才 import torch 和 magic_pdf\n",
    "    import torch\n",
    "    from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\n",
    "    from magic_pdf.data.dataset import PymuDocDataset\n",
    "    from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze\n",
    "    from magic_pdf.config.enums import SupportedPdfParseMethod\n",
    "\n",
    "    try:\n",
    "        if is_file_processed(pdf_file_name, output_dir):\n",
    "            print(f\"Skipping already processed file: {pdf_file_name}\")\n",
    "            return\n",
    "\n",
    "        name_without_suff = os.path.splitext(os.path.basename(pdf_file_name))[0]\n",
    "\n",
    "        local_image_dir = os.path.join(output_dir, \"images\", name_without_suff)\n",
    "        local_md_dir = os.path.join(output_dir, \"markdown\")\n",
    "        os.makedirs(local_image_dir, exist_ok=True)\n",
    "        os.makedirs(local_md_dir, exist_ok=True)\n",
    "\n",
    "        image_writer = FileBasedDataWriter(local_image_dir)\n",
    "        md_writer = FileBasedDataWriter(local_md_dir)\n",
    "\n",
    "        reader = FileBasedDataReader(\"\")\n",
    "        pdf_bytes = reader.read(pdf_file_name)\n",
    "        ds = PymuDocDataset(pdf_bytes)\n",
    "\n",
    "        # 如果 doc_analyze 内部会用到 GPU（例如 model.to(\"cuda:0\")），\n",
    "        # 那么此时进程只看到一块卡，它的 index 就是 0。\n",
    "        # 所以 \"cuda:0\" 会对应系统的物理 GPU #gpu_id。\n",
    "        # 这里无需改动 doc_analyze 的代码，但如果 doc_analyze 里硬编码了 \"cuda:0\"\n",
    "        # 作为全局物理卡，就需要改成对子进程而言的 \"cuda:0\"。\n",
    "\n",
    "        # 推断并分类\n",
    "        parse_method = ds.classify()\n",
    "        if parse_method == SupportedPdfParseMethod.OCR:\n",
    "            infer_result = ds.apply(doc_analyze, ocr=True)\n",
    "            pipe_result = infer_result.pipe_ocr_mode(image_writer)\n",
    "        else:\n",
    "            infer_result = ds.apply(doc_analyze, ocr=False)\n",
    "            pipe_result = infer_result.pipe_txt_mode(image_writer)\n",
    "\n",
    "        # 输出结果文件\n",
    "        pipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", os.path.basename(local_image_dir))\n",
    "        pipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", os.path.basename(local_image_dir))\n",
    "\n",
    "        print(f\"Successfully processed: {pdf_file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file_name}: {e}\")\n",
    "\n",
    "def gather_pdf_files(input_folder):\n",
    "    \"\"\"\n",
    "    扫描文件夹，返回所有 PDF 文件的绝对路径列表。\n",
    "    \"\"\"\n",
    "    pdf_files = []\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_file_path = os.path.join(root, file)\n",
    "                pdf_files.append(pdf_file_path)\n",
    "    return pdf_files\n",
    "\n",
    "def process_chunk(pdf_files_chunk, output_dir, gpu_id):\n",
    "    \"\"\"\n",
    "    子进程函数：\n",
    "    - 先设置子进程可见的 GPU\n",
    "    - 再 import torch (以及 doc_analyze 相关的库) 或在 process_pdf 里再 import\n",
    "    - 然后遍历要处理的 PDF 文件\n",
    "    \"\"\"\n",
    "\n",
    "    # 第一步：告诉当前子进程，只能使用物理 GPU #gpu_id\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "    # 第二步：现在才可以 import torch（如需在此函数用 GPU）\n",
    "    import torch\n",
    "\n",
    "    print(f\"\\n[Process {os.getpid()}] I will use physical GPU={gpu_id}.\")\n",
    "    print(f\"  Before any CUDA ops: torch.cuda.device_count() = {torch.cuda.device_count()}\")\n",
    "\n",
    "    # 如果 torch.cuda 还未初始化，这时 device_count() 可能就是 1，或者仍然是4（取决于实际情况）。\n",
    "    # 要进一步确认可以尝试显式初始化，比如分配一个张量：\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            test_x = torch.randn(1).cuda()\n",
    "            print(\"  GPU test allocation successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  GPU test allocation failed: {e}\")\n",
    "\n",
    "    # 第三步：处理该 chunk 下的所有 PDF 文件\n",
    "    for pdf_file in pdf_files_chunk:\n",
    "        process_pdf(pdf_file, output_dir)\n",
    "\n",
    "    print(f\"[Process {os.getpid()}] Done with GPU {gpu_id}.\\n\")\n",
    "\n",
    "def process_folder(input_folder, output_dir=\"output\", num_gpus=4):\n",
    "    \"\"\"\n",
    "    主调度函数：\n",
    "    1. 收集所有 PDF 文件\n",
    "    2. 按 num_gpus 分成若干个块\n",
    "    3. 多进程并行，每个进程分配到一个 GPU\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_pdf_files = gather_pdf_files(input_folder)\n",
    "    total_files = len(all_pdf_files)\n",
    "    if total_files == 0:\n",
    "        print(\"No PDF files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {total_files} PDF files in '{input_folder}'. Starting processing...\")\n",
    "\n",
    "    # 将 PDF 文件列表拆分为 num_gpus 份\n",
    "    chunk_size = math.ceil(total_files / num_gpus)\n",
    "    pdf_file_chunks = [\n",
    "        all_pdf_files[i : i + chunk_size]\n",
    "        for i in range(0, total_files, chunk_size)\n",
    "    ]\n",
    "\n",
    "    # 创建多进程池并开始并行处理\n",
    "    pool = Pool(processes=num_gpus)\n",
    "    for gpu_id, chunk in enumerate(pdf_file_chunks):\n",
    "        pool.apply_async(process_chunk, (chunk, output_dir, gpu_id))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(\"All processes are done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 第一步：使用 'spawn' 模式启动子进程，以避免在 Linux 下使用 'fork' 导致的 CUDA 上下文继承问题\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    # 你要处理的 PDF 所在文件夹\n",
    "    input_folder = \"/home/yancong/第二次语料库/books/化工设备设计全书/\"\n",
    "    output_dir   = \"/home/yancong/第二次语料库/books/化工设备设计全书/output\"\n",
    "\n",
    "    # 同时使用 4 张 GPU\n",
    "    process_folder(input_folder, output_dir, num_gpus=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62567678-14fe-4685-b0ad-c8f0587b914d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 文本清洗\n",
    "清洗markdown内容并将其转化为txt格式文本，包括降噪以及删除无关内容等功能，后期可以根据要求添加功能\n",
    "使用txtclean（1）.py进行工作\n",
    "使用过程中仅需输入pdf提取过程的输出文件夹即可\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b2280-e23c-4662-b62b-06d3dd22bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_markdown(content):\n",
    "    \"\"\"\n",
    "    清洗 Markdown 内容\n",
    "    - 删除图片和链接标记\n",
    "    - 移除无关元数据（如版权信息、邮箱等）\n",
    "    - 删除目录部分（通过连续的章节标题判断）\n",
    "    - 删除多余空行和尾部空格\n",
    "    - 删除方括号及其内容，以及参考文献\n",
    "    - 删除类似 Bopp(1984), Brown（1983） 的参考文献\n",
    "    \"\"\"\n",
    "    cleaned_content = []\n",
    "    is_in_toc = False  # 标记是否处于目录部分\n",
    "    previous_line = None  # 用于追踪上一行是否为空\n",
    "\n",
    "    for line in content:\n",
    "        # 去除行尾空格\n",
    "        line = line.rstrip()\n",
    "\n",
    "        # 检查是否进入目录部分\n",
    "        if line.lower().startswith(\"contents\") or \"目录\" in line:\n",
    "            is_in_toc = True\n",
    "            continue\n",
    "        # 检查是否结束目录部分（假设目录以空行或某些章节开始）\n",
    "        if is_in_toc and (not line.strip() or line.startswith(\"#\")):\n",
    "            is_in_toc = False\n",
    "        # 跳过目录部分\n",
    "        if is_in_toc:\n",
    "            continue\n",
    "\n",
    "        # 跳过包含图片和链接的行\n",
    "        if \"![\" in line or \"](\" in line:\n",
    "            continue\n",
    "        # 跳过无关的元数据行，如版权、邮箱、网址等\n",
    "        if \"copyright\" in line.lower() or \"email\" in line.lower() or \"www.\" in line.lower():\n",
    "            continue\n",
    "\n",
    "        # 删除方括号及其内容\n",
    "        line = re.sub(r\"\\[.*?\\]\", \"\", line)\n",
    "\n",
    "        # 删除类似 \"Bopp(1984)\", \"Brown（1983）\", \"Brown和Phillips（1989，1991）\" 的内容\n",
    "        line = re.sub(r\"\\b[A-Za-z\\u4e00-\\u9fa5]+(?:和[A-Za-z\\u4e00-\\u9fa5]+)?\\s*[（(]\\d{4}(?:，\\d{4})*[）)]\", \"\", line)\n",
    "\n",
    "        # 删除多余的空行（连续多个空行只保留一个）\n",
    "        if not line.strip():\n",
    "            if previous_line is None or not previous_line.strip():\n",
    "                continue  # 当前行和上一行都是空行时跳过\n",
    "            else:\n",
    "                cleaned_content.append(\"\")  # 保留一个空行\n",
    "        else:\n",
    "            cleaned_content.append(line)  # 添加非空行\n",
    "\n",
    "        # 更新上一行内容\n",
    "        previous_line = line\n",
    "\n",
    "    return cleaned_content\n",
    "\n",
    "def clean_and_extract_markdown(content):\n",
    "    \"\"\"\n",
    "    提取 Markdown 中标题和其下的文本，并清洗标题和正文\n",
    "    - 提取标题及其下的文本\n",
    "    - 删除标题及其下的文本小于等于 20 个字符的部分\n",
    "    - 删除指定标题（如“前言”）和包含关键词（如“思考题”）的段落\n",
    "    - 将文本拼接成长段格式，不换行\n",
    "    \"\"\"\n",
    "    cleaned_content = clean_markdown(content)  # 先执行基本清洗\n",
    "    result = []\n",
    "    current_title = None\n",
    "    current_text = []\n",
    "    skip_section = False  # 标记是否跳过当前段落\n",
    "\n",
    "    for line in cleaned_content:\n",
    "        # 判断是否是标题（假设标题为 Markdown 标题格式，如 # 或 ## 开头）\n",
    "        if line.startswith(\"#\"):\n",
    "            # 如果已有标题和文本，进行处理\n",
    "            if current_title and current_text:\n",
    "                combined_text = \"\".join(current_text).strip()\n",
    "                # 检查标题下的文本长度是否超过 20\n",
    "                if not skip_section and len(re.findall(r\"[\\u4e00-\\u9fa5\\w]+\", combined_text)) > 20:\n",
    "                    result.append(f\"{current_title}\\n{combined_text}\\n\")\n",
    "\n",
    "            # 更新当前标题，清空文本，并检查是否需要跳过\n",
    "            current_title = line.strip()\n",
    "            current_text = []\n",
    "            skip_section = any(keyword in current_title for keyword in [\"前言\", \"思考题\", \"参考文献\"])\n",
    "        else:\n",
    "            # 累积当前标题下的文本\n",
    "            if not skip_section and line.strip():  # 忽略空行\n",
    "                current_text.append(line.strip())\n",
    "\n",
    "    # 处理最后一个标题和文本\n",
    "    if current_title and current_text:\n",
    "        combined_text = \"\".join(current_text).strip()\n",
    "        if not skip_section and len(re.findall(r\"[\\u4e00-\\u9fa5\\w]+\", combined_text)) > 20:\n",
    "            result.append(f\"{current_title}\\n{combined_text}\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_markdown_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    处理文件夹内的所有 Markdown 文件并保存成 .txt 文件\n",
    "    - 读取每个 Markdown 文件\n",
    "    - 清洗并提取标题及其下的文本\n",
    "    - 清洗后保存为 .txt 文件\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        if os.path.isfile(input_file_path) and filename.endswith('.md'):\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.readlines()\n",
    "\n",
    "            # 清洗并提取内容\n",
    "            cleaned_content = clean_and_extract_markdown(content)\n",
    "\n",
    "            # 输出结果到目标文件夹\n",
    "            if cleaned_content:  # 只有当内容非空时才保存文件\n",
    "                output_file_path = os.path.join(output_folder, filename.replace('.md', '.txt'))\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as cleaned_file:\n",
    "                    cleaned_file.write(\"\\n\".join(cleaned_content))\n",
    "                print(f\"Processed and saved: {output_file_path}\")\n",
    "            else:\n",
    "                print(f\"Skipped empty result for: {input_file_path}\")\n",
    "\n",
    "\n",
    "# 示例：读取 input_folder 内的文件并输出到 output_folder\n",
    "input_folder = r\"G:\\yuliaoku\\新能源热利用与热发电原理及系统\\output\\markdown\"\n",
    "output_folder = r\"G:\\yuliaoku\\新能源热利用与热发电原理及系统\\cleaned_markdown_files_txt\"\n",
    "\n",
    "process_markdown_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0611d-1d19-42d8-81c7-3fa5d787345e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 文本合并及JSONL的转化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16ff99-0bee-4e5a-b552-abf1ec5d8b06",
   "metadata": {},
   "source": [
    "## 文本合并\n",
    "使用me.py进行文本合并，输入路径为txt清理的输出路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42718e-b8e1-4240-888d-c398f194eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置输入文件夹和输出文件路径\n",
    "input_folder = r\"G:\\yuliaoku\\1-quan\\课程\" # 替换为您的文件夹路径\n",
    "output_file = r\"G:\\yuliaoku\\1-quan\\课程/未去重.txt\"  # 替换为合并后的文件路径\n",
    "\n",
    "# 创建或清空输出文件\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    # 遍历文件夹中的所有 .txt 文件\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            print(f\"正在处理文件: {file_path}\")\n",
    "            # 读取每个文件并写入输出文件\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")  # 添加换行符以分隔文件内容\n",
    "\n",
    "print(f\"所有 .txt 文件已合并到 {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f220da9-b71a-4244-99e2-ff0fe7df7cb4",
   "metadata": {},
   "source": [
    "## jsonl的转化\n",
    "使用jsonl.py进行转化，输入文件为me.py合并的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738f65b-b05f-4f0e-83a7-43402ce873a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def txt_to_jsonl(input_folder, output_folder):\n",
    "    # 确保输出文件夹存在\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 遍历输入文件夹中的所有txt文件\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(input_folder, file_name)\n",
    "            output_file_name = file_name.replace(\".txt\", \".jsonl\")\n",
    "            output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            jsonl_data = []\n",
    "            section_title = None\n",
    "            content = []\n",
    "\n",
    "            for line in lines:\n",
    "                if line.startswith(\"#\"):  # 标题行\n",
    "                    if section_title:\n",
    "                        jsonl_data.append({\"section\": section_title, \"content\": \"\".join(content).strip()})\n",
    "                    section_title = line.strip(\"#\").strip()\n",
    "                    content = []\n",
    "                else:\n",
    "                    content.append(line)\n",
    "\n",
    "            # 添加最后的部分\n",
    "            if section_title:\n",
    "                jsonl_data.append({\"section\": section_title, \"content\": \"\".join(content).strip()})\n",
    "\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                for entry in jsonl_data:\n",
    "                    f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "            print(f\"Converted: {file_name} -> {output_file_name}\")\n",
    "\n",
    "\n",
    "# 设置输入和输出文件夹路径\n",
    "input_folder = r\"G:\\yuliaoku\\jsonl\" # 替换为您的输入文件夹路径\n",
    "output_folder =r\"G:\\yuliaoku\\jsonl\"  # 替换为您的输出文件夹路径\n",
    "\n",
    "# 执行转换\n",
    "txt_to_jsonl(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30caf146-e89d-4035-8b5d-6cb47b3fc19e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 文本去重\n",
    "根据sentence-bret模型根据语义去重，可以调整similarity_threshold=0.8的值来设置去重力度，越接近0去重力度越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713a4fc-df6a-4a98-9e4b-c3fe06b359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def semantic_deduplicate(input_file, output_file, model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                         similarity_threshold=0.9, batch_size=1024, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    使用Sentence-BERT模型对文本进行语义去重的基础示例。\n",
    "\n",
    "    参数说明：\n",
    "    - input_file: 输入文本文件，每行一条文本记录\n",
    "    - output_file: 输出去重后的文本文件\n",
    "    - model_name: SentenceTransformer可加载的模型名称\n",
    "    - similarity_threshold: 相似度阈值（0~1之间的余弦相似度）\n",
    "    - batch_size: 批处理大小，每次计算多少行的嵌入\n",
    "    - encoding: 文件编码\n",
    "\n",
    "    步骤：\n",
    "    1. 批量读取文本行并计算嵌入向量\n",
    "    2. 对每行文本向量与已保留向量集计算相似度，若最高相似度低于阈值则保留该行并添加其向量到集合中。\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # 用于保存已保留行的向量\n",
    "    retained_embeddings = []\n",
    "    retained_texts = []\n",
    "\n",
    "    with open(input_file, 'r', encoding=encoding) as fin, \\\n",
    "            open(output_file, 'w', encoding=encoding) as fout:\n",
    "\n",
    "        buffer_lines = []\n",
    "        line_count = 0\n",
    "        unique_count = 0\n",
    "\n",
    "        for line in tqdm(fin, desc=\"Processing\"):\n",
    "            text = line.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            buffer_lines.append(text)\n",
    "\n",
    "            # 批处理，当积累到一定数量时才计算嵌入\n",
    "            if len(buffer_lines) >= batch_size:\n",
    "                unique_lines = process_batch(buffer_lines, model, retained_embeddings, retained_texts,\n",
    "                                             similarity_threshold)\n",
    "                # 将去重后保留的行写入文件，并更新retained_embeddings\n",
    "                for t, emb in unique_lines:\n",
    "                    fout.write(t + \"\\n\")\n",
    "                    retained_embeddings.append(emb)\n",
    "                    retained_texts.append(t)\n",
    "                unique_count += len(unique_lines)\n",
    "                line_count += len(buffer_lines)\n",
    "                buffer_lines = []\n",
    "\n",
    "        # 处理剩余行\n",
    "        if buffer_lines:\n",
    "            unique_lines = process_batch(buffer_lines, model, retained_embeddings, retained_texts, similarity_threshold)\n",
    "            for t, emb in unique_lines:\n",
    "                fout.write(t + \"\\n\")\n",
    "                retained_embeddings.append(emb)\n",
    "                retained_texts.append(t)\n",
    "            unique_count += len(unique_lines)\n",
    "            line_count += len(buffer_lines)\n",
    "\n",
    "        print(f\"处理完成！共处理 {line_count} 行，最终输出 {unique_count} 行。\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def process_batch(lines, model, retained_embeddings, retained_texts, similarity_threshold):\n",
    "    \"\"\"\n",
    "    对一批文本行进行嵌入计算，然后对每行查找与已保留文本的最高相似度，决定是否保留。\n",
    "    返回 (text, embedding) 的列表。\n",
    "    \"\"\"\n",
    "    if len(lines) == 0:\n",
    "        return []\n",
    "\n",
    "    embs = model.encode(lines, convert_to_numpy=True)\n",
    "    # 归一化，使向量长度为1\n",
    "    embs = embs / np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "    results = []\n",
    "    # 若尚无已保留文本，全部保留\n",
    "    if len(retained_embeddings) == 0:\n",
    "        for i, line in enumerate(lines):\n",
    "            results.append((line, embs[i]))\n",
    "        return results\n",
    "\n",
    "    # 将已保留的嵌入合并为矩阵\n",
    "    retained_matrix = np.vstack(retained_embeddings) if len(retained_embeddings) > 0 else None\n",
    "    for i, line in enumerate(lines):\n",
    "        emb = embs[i]\n",
    "        # 计算与已保留嵌入的相似度（点积即为余弦相似度，因为已归一化）\n",
    "        scores = np.dot(retained_matrix, emb)\n",
    "        max_score = np.max(scores) if len(scores) > 0 else 0\n",
    "        if max_score < similarity_threshold:\n",
    "            # 没有相似度超过阈值，保留该文本\n",
    "            results.append((line, emb))\n",
    "        # 否则不保留\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_path = r\"G:\\yuliaoku\\1-quan\\课程/未去重.txt\"\n",
    "    output_path =  r\"G:\\yuliaoku\\1-quan\\课程/去重.txt\"\n",
    "    semantic_deduplicate(input_path, output_path, similarity_threshold=0.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
